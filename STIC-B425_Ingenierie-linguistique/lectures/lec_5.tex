\lecture{5}{samedi 22 février 2020}
\vspace{-1.2cm}

\section{Evaluation de la TA}

Evaluation de la TA
    Un des problèmes les plus traités
    Beaucoup d’impact
    Problème très difficile (subjectivité du langage)

Types d’évaluation
    Humaine
    Automatique (avec référence(s) humaine(s))
    Automatique (sans référence) («quality estimation»)

    Objectif: Trouver la méthode la moins subjective, la plus rapide et la moins chère possible

\subsection{Évaluation Humaine}

Demande à des juges humains d’évaluer une traduction, phrase par phrase.

Question courante: faut-il montrer les phrases dans le contexte ou pas?

Critères de qualité:
    Fidélité («adequacy»)
        Est-ce que le sens est correct ?
        Se fait toujours avec la langue source.
    Fluidité («fluency»)
        Est-ce que la phrase se lit bien ?
        Peut se faire sans la langue source.

    Ces deux critères sont parfois combinés, on mesure si la traduction est correcte

Comment évaluer?

\subsubsection{Jugement intuitif}

On demande un jugement intuitif sur la qualité de la traduction, avec une échelle (par exemple Koehn)

\begin{table}
    \begin{tabular}{|l|l|}
    \hline
    Fluency          & Adequacy       \\ \hline
    flawless         & all meaning    \\ \hline
    good             & most meaning   \\ \hline
    non-native       & much meaning   \\ \hline
    disfluent        & little meaning \\ \hline
    incomprehensible & none           \\ \hline
    \end{tabular}
\end{table}


\textbf{Quelle échelle ?}
Expériences avec 3 à 10 échelons dans la littérature
Problème avec une échelle trop petite / grande ?
Problème avec des échelles impaires ?

\textbf{Nombre de juges}
Accord entre juges («inter-agreement») (score Kappa)
plus il y a d’accord, plus les résultats sont fiables
Que pourrait-on faire si on a un nombre impair de juges ?

\textbf{Profil des juges}
Traducteurs ou utilisateurs (médecins, informaticiens) ?
Beaucoup d’expériences avec le crowdsourcing
Juges monolingues ou bilingues ?

\textbf{Résumé}
Permet d’obtenir une idée de la qualité de la traduction (avec une échelle)
Mais : beaucoup de variation entre les juges, surtout non-traducteurs

\subsubsection{Méthode comparative}

«Ranking» (Koehn)
On compare deux traductions
L’une des deux traductions est-elle meilleure que l’autre ?
Si oui = 1, si non = 0

Mêmes questions que pour le jugement intuitif
Quels critères
avec et sans LS
Quelle échelle
Quels évaluateurs et combien ?
Important de changer l’ordre de présentation des phrases

Plus d’accord entre les juges qu’avec la méthode précédente
Tâche bcp plus simple
N’exige pas nécessairement des traducteurs
Convient bien pour le crowdsourcing
Limites ?

\subsubsection{Nombre d'erreurs}

Nombre d’erreurs à corriger pour avoir traduction parfaite :
Remove all the safety pins from the reservoir -> retirer toutes les goupilles de sécurité du réservoir
Syst. ling. : Retirez{*éliminez} chaque goupille de sécurité du réservoir.
Syst. stat. : Retirez tous les goupille de sécurité du réservoir

Poids différent pour les erreurs, soit en fonction du type d’erreurs ou éventuellement du type de correction :
Correction d’un groupe de plus d’un mot (sg -> pl) : 2
Sélection d’un mot dans une liste : 0.5

Standards:
    MQM le plus utilisé
    SAE très orienté industrie spécialisée

Calcul de taux d'erreur
Nombre d'erreurs de type X multiplié par le poids de l'erreur (selon l'échelle du standard). On somme au final tous les scores et on divise par le nombre total de mots.

Taux d'erreurs
    Très précis
    Mais
        Très long
        Très peu d'accord entre les juges
        Poids a beaucoup d'importance -> subjectivité

Critère: compréhensibilité
    Questionnaires de compréhension
    Gap filling
    "Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating machine translation for gisting"

Évaluation humaine: résumé

Manière naturelle de procéder et reste la référence, mais :
    Subjectivité: peu d'accord entre les juges
    Coût important: il faut en général payer les juges
    Temps
Intéressant d'avoir une méthode automatique

\subsection{Évaluation automatique}

Comparer automatiquement les traductions avec une ou plusieurs références (métriques)

\subsubsection{Rappel et Précision}

Basées sur le mot
Proviennent du domaine de la recherche d’informations
Rappel: Complétude : rép. correctes / total réponses correctes (silence, faux négatifs)
Précision: Le logiciel est-il précis ? Rép. Correctes / total réponses données (bruit, faux positifs)

F-measure: moyenne des 2 mesures

$$F-measure = \frac{correct}{(output-length + reference-length)/2}$$

\subsubsection{WER (word error rate)}

Bcp utilisé en reconnaissance vocale
Basé sur le mot, mais prend en compte l’ordre des mots
Nombre d’étapes d’édition minimales nécessaires pour arriver à la référence (insertions, suppressions, substitutions) / nombre de mots de la référence

$$WER = \frac{S+D+I}{N}$$

S: substitution
D: deletion
I: insertion
N: number of words (in reference)

\subsubsection{Bleu}

Calcul de la précision au niveau des N-grammes
Combien de N-grammes en commun entre la traduction et la référence ?
Bleu 2, Bleu 3, Bleu 4,...
Score entre 0 et 1
On peut faire des "Bleus cumulés" (prends en compte les bleus de niveau inférieur)

Améliorations possibles:
- Plusieurs références possibles
- Ajout d’une pénalité si la phrase traduite est plus courte que la référence (brevity penality)
- Calcul de précision revisitée

Avantages:
- Mesure si la traduction est fluide et fidèle, contrairement à des métriques basées sur le mot
- Semble souvent donner des corrélations avec les jugements humains
- Peut être intéressant pour comparer différentes versions d’un système

Limites:
- Que veut dire le Bleu score ?
- Dépend beaucoup de la référence et nécessite si possible plusieurs références (couteux)
- Favorise les systèmes SMT vs NMT (see Volkaert as reference)
- Attention à la ponctuation, tags, etc.
    L’orage (un mot VS deux mots)

Amélioration:
- Comparaison au niveau de la forme de surface des mots suffisante ? (METEOR) (prendre en compte la forme de base du mot et les synonymes, e.g. pas de score zero entre safety et security)
- Tous les N-grammes ont-ils le même poids? (NIST)

\subsubsection{Conclusion sur l'évaluation}

- Se méfier des évaluations automatiques, même si c’est la norme
- Toujours comparer l’évaluation humaine et automatique
- Vérifier l’accord en juges et la corrélation entre les évaluations

\subsection{Quality estimation}

Cherche à indiquer automatiquement la qualité de la traduction automatique (pour différentes tâches), sans utiliser de référence
    TA : bonne ou pas ?
    Niveau du texte ou de la phrase
Scénario typique : Traduction humaine ou post-édition

Utilise l’apprentissage automatique
    - part d’exemples de bonnes et mauvaises traductions
    - on construit automatiquement le classifieur, qui va classer de nouvelles phrases/documents en utilisant différentes propriétés (traits)  appris des données d’entrainement (mais si la phrase est plus longue que X mots, c’est difficile)

Traits
    - Modèles /Système
    - Textes (avec ou sans analyse syntaxique)
    - Trouver la meilleure combinaison

Critères texte source
1. nombre de mots dans le segment source ;
2. longueur moyenne des phrases
4. nombre de N-grammes rares (mots, POS) dans le corpus source
5. nombre de mots qui ne contiennent pas a-z
6. nombre de signes de ponctuations
7. nombre de mots avec plusieurs catégories grammaticales

Critères texte cible
1. différence nombre de mots sources et cibles
2. différences au niveau des virgules
3. différences au niveau du nombre de verbes
4. nombre moyen de traductions pour les mots de la phrase cible
5. nombre d’alignements possibles
6. séquences rares dans le corpus cible

Limitations
- Tâche difficile car plusieurs traductions possibles
- Peu de données annotées (Shared task)
- Marche mieux au niveau du texte
- Classification souvent binaire

(Lire Specia)
(Lire chapitre de livre de Koehn)

\section{Postédition (PE)}

Correction de la TA par des êtres humains
The “term used for the correction of machine translation output by human linguists/editors” (Veale and Way, 1997)

Véritable métier qui se développe
De plus en plus de formations reconnues
Certification SDL
15\% des diplômés faisaient de la post-édition en 2017

Post-édition vs. révision
La post-édition $\neq$ révision des traductions
“A process of modification rather than revision” (Loffler‐Laurian 1985)
Mossop, 2001
Les réviseurs vérifient la fluidité, les post-éditeurs la fidélité

Différence PE - révision

- Types de fautes ne sont pas identiques
    - TA
        - Fautes plus répétitives (Mots inconnus)
        - Plus de contre-sens, de non-sens et de fautes de vocabulaires
    - TH
        - Fautes typographiques
        - Fautes cognitives (e.g. Norvège vs Suède)

- Il faut faire le moins de corrections possibles vs la meilleure qualité possible -> il faut avant tout rester rentable
“All translation firms together are able to translate far less than 1\% of relevant content produced everyday” (Graça, EAMT 2017)

Faits sur la post-édition

La post-édition ferait gagner du temps, pour une qualité qui est équivalente
20\% et 40\% (2010, Masselot et Plitt, Green et al., 2013)

Les meilleurs post-éditeurs seraient les traducteurs professionnels (Almeida et O’Brien, 2010 ; Carl et Buch-Kromann, 2010)
    - Les plus rapides
    - Plus de changements

Beaucoup de variations entre les post-éditeurs (3000 à 9000 mots par jour)

Types de post-édition

Deux types de post-édition en fonction de la qualité souhaitée
    - Minimale (light, rapid)
        - pour rendre le texte compréhensible
        - Forums, textes communautaires, email, localisation, etc.
    - Complète (full) :
        - plus de corrections -> qualité équivalente à celle d’un texte traduit humainement

Autres types de post-édition
    Bilingue
        Avec la source
    Monolingue
        Sans la source
        Convient surtout pour la post-édition rapide
        Tâche facile pour le crowdsourcing (Unbabel)
    Peuvent être combinées (Morita/Ishida)

Règles générales
    - Garder le plus possible la traduction automatique
    - Ne pas passer trop de temps sur un problème, il vaut mieux retraduire la phrase
    - Ne pas passer de temps sur le style
    - Corriger ce qui est incompréhensible (sens)

Standards (guidelines): Machine Translation Post-Editing Guidelines (TAUS)

Plusieurs questions soulevées
- Comment mesurer l’effort de post-édition ?
- Différences TA post-éditée vs non post-éditées ?
- Comment aider le post-éditeur ?

Effort de postédition
Comprend trois types d’effort, qui interagissent
    - de durée
    - cognitif
    - technique (Krings, 2001)

Comment mesurer l’effort ?
- Temps requis pour post-éditer un texte vs traduire
    - mais bcp. de différence entre personnes
- Nombre d’opérations au clavier (keystrokes, etc.)
    - mais cert. opérations sont plus compliquées que d’autres
- Nombre d’erreurs à corriger dans la TA
    - mais texte de qualité moyenne serait plus difficile à post-éditer qu’un texte de mauvaise qualité (Krings)
    - Certaines fautes sont plus problématiques que d’autres
- Pauses, mouvement des yeux, etc.

Métrique automatique : TER
- On compare la traduction automatique (HYP) et la phrase post-éditée (REF)
- Nombre minimum d’éditions (ajouts, suppressions, substitution de mots) pour arriver à la référence
- Similaire au WER, mais on prend aussi en compte le déplacement de séquence.
- Plateforme: Matecat

Différences TA avec la TH ?

Différentes études sur corpus montrent qu’il y a des différences linguistiques (Culo, Martikainen et Kubler, Toral)
    - Moins de foisonnement en post-édition
    - Plus de calques (syntaxiques) (structures identiques)
    - Plus de cognates
    - Plus de fautes graves dans la version finale (Martikainen et Kübler, 2016)


Comment aider le post-éditeur
- Pré-édition
- Interface
- APE

Pré-édition

Simplifier le texte (Plain Langage, FALC)
Réduit les erreurs de TA, mais réduit-elle l’effort ?
    - Moins d’impact sur la NMT que la SMT ou RBMT (Aikawa, Gerlach) - Thèse de Rossetti, 2018, Simplifying, Reading, and Machine Translating Health Content: An Empirical Investigation of Usability
    - Phrases longues, GN, etc.

Interfaces spécifiques
- pour déplacer ou inverser des mots
- choisir entre plusieurs alternatives
- post-édition interactive (DeepL)

Lien avec autres technologies
Intégration avec :
    - les mémoires de TA
        - Combien de traductions ? \% de correspondance ?
    - la reconnaissance vocale
        - http://www.aclweb.org/anthology/W14-0315
    - les correcteurs ou outils spécifiques

APE (Automatic post-editing)
- Les correcteurs ne sont pas très utiles, mais il faudrait refaire l’étude (Stymme and Ahrenberg, http://stp.lingfil.uu.se/~sara/papers/scl24.pdf)
- Possibilité d’écrire des expressions régulières pour corriger les fautes les plus fréquentes (Guzman)
- Les corrections sont directement intégrées dans les modèles

Expressions régulières (REGEX !!!)
- Suite de caractères typographiques, appelée «pattern» en anglais
- But :
    - décrire des ensembles de chaînes de caractères grâce à des caractères spéciaux
    - rechercher / remplacer

Impact positif
- Gain de productivité
    - plus de contenu traduit
- Moins de répétitions pour le traducteur
- De nouvelles débouchés -> recherche en postédition
- Diversification du métier
- Standardisation (termino, style)

Impact négatif
- On demande plus de productivité au traducteur
- Prix de la traduction diminue
- Moins de créativité
- Différents niveaux de post-édition, donc qualité variable de la traduction
- Erreurs répétitives
