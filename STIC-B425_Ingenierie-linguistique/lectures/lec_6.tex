\lecture{6}{mercredi 04 mars 2020}
\vspace{-1.2cm}

\section{Aspects morpho-lexicaux}

\epigraph{Language is mankind's greatest invention \\- except, of course, that it was never invented!}{\textit{G. Deutscher, 2005, in "The unfolding of language"}}

\textbf{Langages rationnels vs naturels}

La grammaire naturelle est composées de règles MAIS contient de nombreuses exceptions. Il est difficile (si pas impossible) d'anticiper toutes les évolutions de la langue:

\begin{itemize}
    \item pluriels irréguliers
    \item néologismes
    \item langages SMS/tweets
    \item jargon technique
    \item ...\\
\end{itemize}

\textbf{Abstraction}

Pour formaliser le langage naturel il est nécessaire de faire abstraction de nombreuses richesses. Ces simplifications sont nécessaires mais réductrices. Elles sont souvent limitées à un domaine précis et clos:

\begin{itemize}
    \item météo
    \item réservations
    \item ELIZA
    \item ...\\
\end{itemize}

\textbf{Exemples de langages rationnels}

Parler mouton: bê, bêê, bêêê, bêêêê

Rires: haha!, hihihi, hahahaha!

\subsection{Grammaires régulières}

Règles de réécriture: constituant $\rightarrow$ constitué

Majuscules = symboles; minuscules = littéraux

Règles récursives: même symbole utilisé à gauche et à droite de la flèche de réécriture\\

Exemple du parler mouton:

$ X \rightarrow bY $

$ Y \rightarrow \hat e Y $

$ Y \rightarrow \hat e $

\subsection{Expressions régulières (regex)}

Permettent d'exprimer des grammaires régulières de manière plus concise.

Sujet complexe et vaste, syntaxe ésotérique.\\

Exemples:

/bê+/ (parler mouton)

/h[ai](h[ai])+  !?/ (rires)

/\textbackslash b[A-Z0-9.\_\%+-]+@[A-Z0-9-.]+\textbackslash .[A-Z]\{2,\}\textbackslash b/ (détection d'adresse email)\\

Exercices online: \url{http://regex.sketchengine.co.uk/}

Solutions:

1 - p.t /
2 - ap.?t /
3 - af+g.?k /
4 - [a-z][.?!]['")]? +[A-Z]

\subsection{Automates finis}

FSA: Finite-State Automata (pluriel de Automaton)

Automate à nombre fini d'états (pas à états finis!)

Les transitions d'un état à un autre sont soumises à la satisfaction d'une condition.

\vspace{0.5cm}

\begin{center}
\resizebox{0.50\textwidth}{!}{
    \input{images/lec6_tikz1.tex}
}
\end{center}

\vspace{0.5cm}

\begin{minipage}[t]{0.6\textwidth}
États: représentés par des cercles

- État initial (triangle/flèche isolée)

- États intermédiaires

- État final (double cercle)\\
\end{minipage}
\begin{minipage}[t]{0.8\textwidth}
Transitions: représentées par des flèches

- D'un état à un autre

- D'un état à lui-même (récursif)

- Transition lambda/epsilon (vide)\\
\end{minipage}

\begin{center}
\resizebox{0.6\textwidth}{!}{
    \input{images/lec6_tikz2.tex}
}
\end{center}

\textbf{Déterministe vs non-déterministe}

Après avoir reconnu "abb", sommes-nous à l'état final ou bien toujours à l'état initial? Impossible à dire avec certitude.

\begin{center}
\resizebox{\textwidth}{!}{
    \input{images/lec6_tikz3.tex}
}
\end{center}

\newpage

\subsection{Équivalences}

On a décrit trois manières d'exprimer la même chose: les langages rationnels/réguliers/reconnaissables.

\begin{center}
\resizebox{0.6\textwidth}{!}{
    \input{images/lec6_tikz4.tex}
}
\end{center}

\subsection{Morphologie}

La morphologie permet soit de gérer des variantes de mots (morphologie flexionelle) soit de construire des nouveaux mots. \\

Au niveau de la construction, on peut soit faire une dérivation ("affixation", en conservant ou changeant la catégorie du mot) soit faire de la composition (noms composés, adjectifs composés, verbes composés, adverbes composés). \\

\subsection{Transducteurs}

FST: Finite-state transducers.\\

Similaires aux FSA mais la condition pour passer d'un état à un autre est remplacée par une action à effectuer. Utilisés soit pour analyser une chaine existante, soit pour générer une chaîne à partir de traits.\\

\subsection{Stemming}

\textbf{Racinisation (stemming)}

Version simplifiée de l'analyse morphologique. Ne s'encombre pas du lexique: principalement basée sur la désuffixation (vs lemmatisation). Moins précise mais beaucoup plus rapide. Très utilisée par les moteurs de recherche.\\

\textbf{Porter stemmer}

Crée par Martin Porter (1980). Ensemble de règles simples en cascade, par exemple:

\begin{itemize}
    \item SSES $\rightarrow$ SS
    \item ING $\rightarrow$ epsilon
    \item ATIONAL $\rightarrow$ ATE\\
\end{itemize}

Mais, ce n'est pas hyper précis, il y a de nombreux problèmes, comme par exemple:

\begin{itemize}
    \item organization $\rightarrow$ organ (sens perdu)
    \item policy $\rightarrow$ police (sens perdu)
    \item Adobe Illustrator $\rightarrow$ illustrate (nom propre a un sens particulier)\\
\end{itemize}

\subsection{Segmentation (tokenization)}

Séparation d'un texte en phrases et des phrases en mots. Étape cruciale pour le succès des stades suivants (analyse morphologique, traduction automatique, etc).\\

\textbf{Séparation en phrases}

Approche naïve: séparation par un point suivi d'un espace. Mais malheureusement cela ne couvre absolument pas tous les cas.\\

\textbf{Parts of Speech}

Les petites poules du couvent les couvent bien \\

\begin{minipage}[t]{0.5\textwidth}
\begin{itemize}
\item Les: DET
\item petites: ADJ
\item poules: NOM
\item du: PREP
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{itemize}
 \item couvent: NOM
\item les: PRON
\item couvent: VB
\item bien: ADV
\end{itemize}
\end{minipage}

\subsection{Tagging}

\textbf{Tagsets}

Listes des "parties du discours" (catégories)\\

Granularités très variables:
\begin{itemize}
    \item French Treebank: 15 catégories grammaticales
    \item Penn Treebank: 36 (45 avec la ponctuation)
    \item C5: 67
    \item Brown: 87
    \item Sinica (chinois): 294!\\
\end{itemize}

\textbf{PoS tagging}

Étiquetage morpho-syntaxique/grammatical.

Trois approches principales:
\begin{itemize}
    \item linguistiques (règles)
    \item statistique (HMM - hidden markov models, Viterbi)
    \item transformative (Brill tagger)\\
\end{itemize}

\textbf{Taggers linguistiques}

Choix parmi les différents candidats trouvés dans un dictionnaire. Basés sur une succession de règles très complexes. Exemple pour l'anglais: ENGTWOL.\\

\textbf{HMM taggers}

Approche probabiliste basée sur le modèle bayésien d'inférence (Hidden Markov Model - 1763). Choix de la séquence de tags la plus probable parmi toutes les séquences possibles, en fonction des mots observés.

Prémisses simplificatrices: la probabilité d'un mot ne dépend pas du contexte (faux, mais c'est pour simplifier); la probabilité d'un tag ne dépend que du précédent (modèle des bigrammes)

Approximation = produit entre: la probabilité d'un mot hors contexte (fréquence) pour un tag donné; la probabilité de transition d'un tag à un autre\\

\textbf{Brill tagger}

Approche hybride basée sur des règles ET un entrainement statistique (machine learning). Technique supervisée: nécessite un corpus annoté manuellement pour l'apprentissage. Première approximation probabiliste puis règles de transformation.
